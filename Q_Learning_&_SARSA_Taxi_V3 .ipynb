{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"f2X1v82pVbbG","executionInfo":{"status":"ok","timestamp":1699523680885,"user_tz":-330,"elapsed":492,"user":{"displayName":"MOHAMED HASAN FARIS M","userId":"07393633362063938780"}}},"outputs":[],"source":["import numpy as np\n","import gym"]},{"cell_type":"code","source":["def eps_greedy(Q, s, eps=0.1):\n","    if np.random.uniform(0,1) < eps:\n","      return np.random.randint(Q.shape[1])\n","    else:\n","      return greedy(Q, s)"],"metadata":{"id":"pxw5anUxWQX4","executionInfo":{"status":"ok","timestamp":1699523682532,"user_tz":-330,"elapsed":5,"user":{"displayName":"MOHAMED HASAN FARIS M","userId":"07393633362063938780"}}},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":["# Greedy Policy\n","> Returining TO Maximum Action State Value"],"metadata":{"id":"M2dbjgt8dL34"}},{"cell_type":"code","source":["def greedy(Q, s):\n","    return np.argmax(Q[s])"],"metadata":{"id":"M0vffW7OXh0k","executionInfo":{"status":"ok","timestamp":1699523686795,"user_tz":-330,"elapsed":370,"user":{"displayName":"MOHAMED HASAN FARIS M","userId":"07393633362063938780"}}},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":["# Policy Testing\n"],"metadata":{"id":"xFSRuP_FcrQY"}},{"cell_type":"code","source":["def run_episodes(env, Q, num_episodes=100, to_print=False):\n","    tot_rew = [] #total reward\n","    state = env.reset()\n","\n","    for _ in range(num_episodes):\n","        done = False\n","        game_rew = 0\n","\n","        while not done:\n","            next_state, rew, done, _ =env.step(greedy(Q, state))\n","\n","            state = next_state\n","            game_rew += rew\n","            if done:\n","                state = env.reset()\n","                tot_rew.append(game_rew)\n","\n","    if to_print:\n","        print('Mean score: %.3f of %1 games!'%(np.mean(tot_rew), num_episodes))\n","\n","    return np.mean(tot_rew)"],"metadata":{"id":"PSfTY4VsXzWO","executionInfo":{"status":"ok","timestamp":1699523690307,"user_tz":-330,"elapsed":402,"user":{"displayName":"MOHAMED HASAN FARIS M","userId":"07393633362063938780"}}},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":["# **SARSA**\n","* initialize Q Matrix\n","* Decay The Epsilon Until It Reaches The Threshold\n","* Choose Next Action\n","* SARSA Update\n","* Testing The Policy\n","\n","\n","\n","\n","\n"],"metadata":{"id":"Y3OfFwXMfLBP"}},{"cell_type":"code","source":["def SARSA(env, lr=0.01, num_episodes=10000, eps=0.3, gamma=0.95, eps_decay=0.00005):\n","    nA = env.action_space.n\n","    nS = env.observation_space.n\n","\n","    Q = np.zeros((nS, nA))\n","    games_rewards=[]\n","    test_rewards=[]\n","\n","    for ep in range(num_episodes):\n","        state = env.reset()\n","        done = False\n","        tot_rew = 0\n","\n","        if eps>0.01:\n","            eps -= eps_decay\n","\n","\n","        action = eps_greedy(Q, state, eps)\n","\n","        while not done:\n","            next_state, rew, done, _ = env.step(action)\n","\n","            next_action = eps_greedy(Q, next_state, eps)\n","\n","            #Bellman's Equation\n","            Q[state][action] = Q[state][action] + lr*(rew + gamma*Q[next_state][next_action] - Q[state][action])\n","\n","            state = next_state\n","            action = next_action\n","            tot_rew += rew\n","            if done:\n","                games_rewards.append(tot_rew)\n","\n","        if (ep % 300) == 0:\n","              test_rew =run_episodes(env, Q, 1000)\n","              print(\"Episode:{:5d}  Eps:{:2.4f}  Rew:{:2.4f}\".format(ep, eps, test_rew))\n","              test_rewards.append(test_rew)\n","    return Q"],"metadata":{"id":"2Cv5ILXXdEmu","executionInfo":{"status":"ok","timestamp":1699523695499,"user_tz":-330,"elapsed":687,"user":{"displayName":"MOHAMED HASAN FARIS M","userId":"07393633362063938780"}}},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":["SARSA - TAXI V3 DATA"],"metadata":{"id":"WB_vs7rtMiMZ"}},{"cell_type":"code","source":["if __name__ == '__main__':\n","    env = gym.make('Taxi-v3')\n","    print(\"SARSA\")\n","    Q_sarsa = SARSA(env, lr=0.1, num_episodes=5000, eps=0.4, gamma=0.95, eps_decay=0.001)"],"metadata":{"id":"L1dBSoXoMyqY","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1699523740447,"user_tz":-330,"elapsed":39195,"user":{"displayName":"MOHAMED HASAN FARIS M","userId":"07393633362063938780"}},"outputId":"4f847080-fa30-4c11-b4a1-9122d06c80da"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n","  deprecation(\n","/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n","  deprecation(\n"]},{"output_type":"stream","name":"stdout","text":["SARSA\n","Episode:    0  Eps:0.3990  Rew:-205.4000\n","Episode:  300  Eps:0.0990  Rew:-236.9230\n","Episode:  600  Eps:0.0100  Rew:-201.4490\n","Episode:  900  Eps:0.0100  Rew:-165.9270\n","Episode: 1200  Eps:0.0100  Rew:-72.9890\n","Episode: 1500  Eps:0.0100  Rew:-42.7770\n","Episode: 1800  Eps:0.0100  Rew:-62.2390\n","Episode: 2100  Eps:0.0100  Rew:-15.4300\n","Episode: 2400  Eps:0.0100  Rew:-10.6140\n","Episode: 2700  Eps:0.0100  Rew:-1.8770\n","Episode: 3000  Eps:0.0100  Rew:4.6940\n","Episode: 3300  Eps:0.0100  Rew:5.7650\n","Episode: 3600  Eps:0.0100  Rew:6.9760\n","Episode: 3900  Eps:0.0100  Rew:6.3130\n","Episode: 4200  Eps:0.0100  Rew:6.3490\n","Episode: 4500  Eps:0.0100  Rew:7.4890\n","Episode: 4800  Eps:0.0100  Rew:7.9000\n"]}]},{"cell_type":"markdown","source":["**Q-LEARNING**\n","\n"," * initialize Q Matrix\n"," * Decay The Epsilon Until It Reaches The Threshold\n"," * Choose Next Action\n"," * SARSA Update\n"," * Testing The Policy\n","\n"],"metadata":{"id":"6GgIaIQuV8zD"}},{"cell_type":"code","source":["def Q_Learning(env, lr=0.01, num_episodes=10000, eps=0.3, gamma=0.95, eps_decay=0.00005):\n","    nA = env.action_space.n\n","    nS = env.observation_space.n\n","\n","    Q = np.zeros((nS, nA))\n","    games_rewards=[]\n","    test_rewards=[]\n","\n","    for ep in range(num_episodes):\n","        state = env.reset()\n","        done = False\n","        tot_rew = 0\n","\n","        if eps>0.01:\n","            eps -= eps_decay\n","\n","        while not done:\n","\n","            action = eps_greedy(Q, state, eps)\n","            next_state, rew, done, _ = env.step(action)\n","\n","            #Bellman's Equation\n","            Q[state][action] = Q[state][action] + lr*(rew + gamma*np.max(Q[next_state]) - Q[state][action])\n","\n","            state = next_state\n","            tot_rew += rew\n","            if done:\n","                games_rewards.append(tot_rew)\n","\n","        if (ep % 300) == 0:\n","              test_rew =run_episodes(env, Q, 1000)\n","              print(\"Episode:{:5d}  Eps:{:2.4f}  Rew:{:2.4f}\".format(ep, eps, test_rew))\n","              test_rewards.append(test_rew)\n","    return Q"],"metadata":{"id":"7I_r9Ko4V6ET","executionInfo":{"status":"ok","timestamp":1699523753420,"user_tz":-330,"elapsed":366,"user":{"displayName":"MOHAMED HASAN FARIS M","userId":"07393633362063938780"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["if __name__ == '__main__':\n","    env = gym.make('Taxi-v3')\n","    print(\"Q_Learning\")\n","    Q_Learning = Q_Learning(env, lr=0.1, num_episodes=5000, eps=0.4, gamma=0.95, eps_decay=0.001)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4iu_Ox42RR9p","executionInfo":{"status":"ok","timestamp":1699523794364,"user_tz":-330,"elapsed":37228,"user":{"displayName":"MOHAMED HASAN FARIS M","userId":"07393633362063938780"}},"outputId":"26dbd84f-2218-49d1-8924-5c804af1df58"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Q_Learning\n","Episode:    0  Eps:0.3990  Rew:-275.2310\n","Episode:  300  Eps:0.0990  Rew:-209.6020\n","Episode:  600  Eps:0.0100  Rew:-203.9030\n","Episode:  900  Eps:0.0100  Rew:-186.9950\n","Episode: 1200  Eps:0.0100  Rew:-109.0820\n","Episode: 1500  Eps:0.0100  Rew:-94.1440\n","Episode: 1800  Eps:0.0100  Rew:-29.0390\n","Episode: 2100  Eps:0.0100  Rew:-26.1290\n","Episode: 2400  Eps:0.0100  Rew:-4.1200\n","Episode: 2700  Eps:0.0100  Rew:-0.5320\n","Episode: 3000  Eps:0.0100  Rew:6.6530\n","Episode: 3300  Eps:0.0100  Rew:-3.9140\n","Episode: 3600  Eps:0.0100  Rew:3.7260\n","Episode: 3900  Eps:0.0100  Rew:5.8690\n","Episode: 4200  Eps:0.0100  Rew:-2.1160\n","Episode: 4500  Eps:0.0100  Rew:7.9380\n","Episode: 4800  Eps:0.0100  Rew:7.7880\n"]}]}]}